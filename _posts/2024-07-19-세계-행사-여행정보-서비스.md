---
title: 세계 행사 기준 여행 정보 서비스 후기
tags: Project
---

# 프로젝트 소개

세계의 행사들을 중심으로 근처의 유명 식당, 숙소, 그리고 가까운 공항의 항공권까지 한 번에 검색할 수 있는 프로그램입니다.

- [GitHub 링크](https://github.com/Toursherpa/TourSherpa)
- [Notion 링크](https://www.notion.so/d4bb59baa14e47a9b2c22a3e7960dc43)
- [발표 PPT 링크](https://drive.google.com/file/d/1zhKdh9gckS2sNNlfrvFkWspRoqZk-Wm7/view?usp=sharing)

## 참여 역할
- 팀 매니징
- 세계 행사(축제, 콘서트, 박람회 등) ETL
- ELT DAG 작성
- 전체 차트 대시보드 시각화
- 행사 관련 백엔드, 프론트엔드
- 보고서 작성

---

## 첫 번째 이슈: 대시보드 구성 중 Redshift와 Django의 호환성 문제

### 문제의 배경
- **ORM 호환성 문제**: Django의 ORM이 Amazon Redshift의 데이터 타입 및 SQL 기능을 완벽히 지원하지 않음.
- **드라이버 및 연결 설정 문제**: Redshift와 Django 간의 연결 설정에서 오류가 발생하여 데이터베이스 통합에 어려움이 있었음.

**해결하지 않을 시**: 대시보드 구성이 불가능하여 프로젝트 진행이 지연될 수 있음.

### 시도한 해결 방법
- **RDS로 대체 시도**: Amazon RDS로 대체하여 문제를 해결하려 했으나, 요구 사항에 맞지 않아 실패.
- **`psycopg2-binary` 설치**: 드라이버 문제를 해결하기 위해 설치했으나, 여전히 문제 발생.

---

### 첫 번째 이슈 해결 과정

#### 최종 해결 방법: `django-redshift-backend` 패키지 도입

##### 패키지 역할:
- Redshift의 SQL 쿼리 요구사항에 맞게 ORM을 조정.
- Django의 데이터베이스 설정을 Redshift에 맞게 간소화.

##### 주요 기능:
- Redshift의 데이터 타입과 SQL 기능 지원.
- 커넥션 및 드라이버 설정 최적화.

##### 구체적인 코드 예제:

```python
DATABASES = {
    'default': {
        'ENGINE': 'redshift_backend',
        'NAME': 'our_database_name',
        'USER': 'our_username',
        'PASSWORD': 'our_password',
        'HOST': 'our_redshift_endpoint',
        'PORT': '5439',  # Redshift의 기본 포트
    }
}
```

#### 해결 후 결과
- **통합 성공**: Django 애플리케이션과 Redshift 간의 원활한 데이터베이스 통합이 완료되었습니다.
- **성능 향상**: Redshift에서의 데이터 조회 및 처리 속도가 개선되었고, Django 애플리케이션의 데이터베이스 작업 안정성이 향상되었습니다.

---

### 배운 점 및 교훈

이 과정을 통해 배운 것은 다음과 같습니다:
- **호환성 고려**: 프로젝트의 초기 단계에서 사용하는 기술 스택 간의 호환성을 충분히 검토하는 것이 매우 중요하다는 것을 배웠습니다. Redshift와 Django 간의 호환성 문제는 프로젝트 진행 중간에 발생하여 해결하는 데 많은 시간과 노력이 소요되었습니다. 이를 통해, 프로젝트를 시작할 때 각 기술의 제한 사항과 호환성을 미리 확인하는 것이 중요함을 깨달았습니다.
- **문제 해결 접근법**: 문제 해결 과정에서 다양한 접근법을 시도해보는 것이 중요하다는 것을 배웠습니다. 처음에는 RDS로의 대체를 시도하고, 드라이버 설치 등의 방법을 통해 문제를 해결하려 했으나, 이들이 모두 실패했을 때 문제의 근본 원인을 더 깊이 파악하고, 최적의 솔루션을 찾기 위해 다양한 시도를 해보았습니다. 이 과정에서 실패를 두려워하지 않고, 끊임없이 새로운 방법을 탐색하는 것이 중요하다는 교훈을 얻었습니다.

#### 추가 개선 가능성
- 현재는 문제를 해결했지만, 미래에는 Redshift와 Django의 호환성을 더 원활하게 유지하기 위해 정기적으로 패키지 업데이트 및 최적화 방안을 검토할 필요가 있습니다.

---

### 결론

프로젝트 진행 중 발생한 문제를 해결하는 과정에서 많은 교훈을 얻었으며, 기술 스택 간의 호환성 문제를 해결할 수 있는 방법을 배웠습니다. 이 글이 비슷한 문제를 겪는 분들에게 도움이 되길 바랍니다.

### 추가 자료
- [Django-Redshift-Backend GitHub](https://github.com/jonashagstedt/django-redshift-backend)
- [Amazon Redshift Documentation](https://docs.aws.amazon.com/redshift/index.html)

---

### Q&A

**Q: 왜 django와 redshift 구성을 고집했는가? 다른 방법은 왜 사용하지 않았나?**

**A:** Django와 Redshift의 구성을 고집한 이유는 두 가지 주요 요소가 있습니다. 첫째, 프로젝트는 많은 상세 페이지와 복잡한 기능을 필요로 했습니다. Django는 그 유연성과 강력한 기능 덕분에 복잡한 웹 애플리케이션을 효율적으로 구축할 수 있는 프레임워크로 적합했습니다. Django의 ORM과 관리 인터페이스는 복잡한 데이터 구조와 다양한 요구사항을 처리하는 데 매우 유용했습니다.

둘째, Redshift는 대규모 데이터 처리와 분석에 최적화된 데이터베이스로, 우리가 처리해야 할 방대한 이벤트 데이터를 효율적으로 관리할 수 있는 능력을 제공했습니다. 다른 대체 데이터베이스, 예를 들어 Amazon RDS는 Redshift와 같은 수준의 데이터 처리 능력을 제공하지 못했습니다.


**Q: 다른 방법들(RDS,  psycopg2-binary 드라이버 설치)은 왜 사용하지 않았나?**

**A:** 다른 방법들, 예를 들어 Amazon RDS로의 대체를 시도했지만, Redshift가 제공하는 대규모 데이터 처리와 분석 기능을 대체할 수 없었기 때문에 실패했습니다. 또한, psycopg2-binary 드라이버 설치를 통한 연결 문제 해결도 시도했지만, 이는 호환성 문제를 완전히 해결할 수 없었습니다. 결과적으로, django-redshift-backend 패키지가 Django와 Redshift 간의 호환성을 최적화할 수 있는 가장 적합한 솔루션으로 판단되어 선택하게 되었습니다.

---
## 두 번째 이슈: API 키 한도 제한

### 문제의 배경
- **API 키 한도 제한**: 특정 API 키에 사용 한도가 정해져 있어 프로젝트에 필요한 분량의 데이터를 얻지 못하는 문제가 발생했습니다. 이로 인해 데이터 수집 작업이 원활히 진행되지 않았습니다.

**해결하지 않을 시**: API 사용 한도로 인해 필요한 데이터를 충분히 수집하지 못하면 서비스 품질이 저하되고 사용자 경험이 떨어질 수 있습니다. 이로 인해 프로젝트 일정이 지연될 수 있으며, 추가적인 대체 솔루션이나 개발 작업이 필요해질 수 있습니다.

### 시도한 해결 방법
- **업데이트 날짜 제한**: 데이터 수집 시 API 호출 빈도를 줄이기 위해 업데이트 날짜를 제한했습니다.
- **계정 여러 개 사용**: API 호출 한도를 초과하지 않기 위해 여러 계정을 만들어 호출을 분산시켰습니다.

---

### 두 번째 이슈 해결 과정

#### 최종 해결 방법: 업데이트된 행사 리스트를 따로 생성하고 API 호출 수 줄이기

##### 해결 방법 개요:
- **업데이트된 행사 리스트 생성**: 업데이트된 행사 정보를 별도로 관리하고, 해당 행사와 관련된 정보만 새로 가져오도록 하여 API 호출 수를 줄였습니다.
- **업데이트 타입 구분**: 업데이트의 타입을 `update_type` 컬럼을 사용하여 아래와 같이 구분했습니다

| update_type | 업데이트의 타입 상세 내용                               |
|-------------|-------------------------------------------------------|
| 1           | 이벤트가 업데이트되었으나 위도 경도가 바뀌지 않은 경우  |
| 2           | 이벤트가 업데이트되었고 위도 경도가 바뀐 경우          |
| 3           | 새롭게 추가된 이벤트                                   |


##### 구체적인 코드 예제:

```python
#새로운 데이터와 새로 업데이트된 행사 데이터 가져오기
def fetch_and_upload_data(**kwargs):
    combined_df = pd.DataFrame()
    for country in countrys:
        for category in categories:
            fetch_data = fetch_data_setting(country, category)[0]
            df = pd.DataFrame(fetch_data["results"])
            if not df.empty:
                combined_df = pd.concat([combined_df, df], ignore_index=True)

    if not combined_df.empty:
        combined_df = combined_df.sort_values(by=['rank', 'predicted_event_spend'], ascending=[False, False])
        combined_df['update_type'] = 0
        combined_df.to_csv(f'/tmp/TravelEvent_data.csv', index=False, encoding='utf-8-sig')
    else:
        combined_df.to_csv(f'/tmp/TravelEvent_data.csv', index=False, encoding='utf-8-sig')
        print(f"No data fetched for. Skipping CSV creation.")

    print("Domestic data fetched and saved to '/tmp/TravelEvents_data.csv'")

    combined_up_df = pd.DataFrame()
    for country in countrys:
        for category in categories:
            fetch_up_data = fetch_data_setting(country, category)[1]
            up_df = pd.DataFrame(fetch_up_data["results"])
            if not up_df.empty:
                combined_up_df = pd.concat([combined_up_df, up_df], ignore_index=True)

    if not combined_up_df.empty:
        combined_up_df = combined_up_df.sort_values(by=['rank', 'predicted_event_spend'], ascending=[False, False])
        combined_up_df['update_type'] = 1
        combined_up_df.to_csv(f'/tmp/UP_TravelEvent_data.csv', index=False, encoding='utf-8-sig')
    else:
        combined_up_df.to_csv(f'/tmp/UP_TravelEvent_data.csv', index=False, encoding='utf-8-sig')
        print(f"No data fetched for. Skipping CSV creation.")

    print("Domestic data fetched and saved to '/tmp/UP_TravelEvents_data.csv'")

    kwargs['ti'].xcom_push(key='combined_df_path', value='/tmp/TravelEvent_data.csv')
    kwargs['ti'].xcom_push(key='combined_up_df_path', value='/tmp/UP_TravelEvent_data.csv')

#전날 데이터 가져오기
def read_data_from_s3(**kwargs):
    s3_hook = S3Hook('s3_connection')
    s3_bucket_name = Variable.get('s3_bucket_name')

    s3_key = f'source/source_TravelEvents/{yesterday_str}/TravelEvents.csv'
    if s3_hook.check_for_key(key=s3_key, bucket_name=s3_bucket_name):
        file_obj = s3_hook.get_key(key=s3_key, bucket_name=s3_bucket_name)
        file_content = file_obj.get()['Body'].read().decode('utf-8')
        transformed_data = {'file_content': file_content}
    else:
        print("파일을 찾을 수 없습니다. 건너뜁니다.")
        transformed_data = None

    kwargs['ti'].xcom_push(key='s3_data', value=transformed_data)

'''
전날 데이터와 최신 데이터로 update_type을 기반으로 한 update csv 파일 업데이트
0-동일한 데이터, 변화 X
1-행사 위치가 아닌 다른 데이터의 변화 O
2-행사 위치를 포함한 데이터의 변화 O
3-새로운 EventID 생성됨 O
'''
def update_combined_df(**kwargs):
    ti = kwargs['ti']
    combined_df_path = ti.xcom_pull(task_ids='fetch_data_TravelEvents', key='combined_df_path')
    combined_up_df_path = ti.xcom_pull(task_ids='fetch_data_TravelEvents', key='combined_up_df_path')

    combined_df = pd.read_csv(combined_df_path)
    combined_up_df = pd.read_csv(combined_up_df_path)

    s3_data = kwargs['ti'].xcom_pull(key='s3_data', task_ids='read_data_from_s3')

    if s3_data and 'file_content' in s3_data:
        file_content = s3_data['file_content']
        pre_df = pd.read_csv(StringIO(file_content))
        pre_df['update_type'] = 0

        combined_ids = set(combined_df['id'])
        pre_ids = set(pre_df['id'])
        new_data = combined_df[combined_df['id'].isin(combined_ids - pre_ids)]
        new_data['update_type'] = 3

        merged_df = pd.merge(combined_up_df, pre_df[['id', 'location']], on='id', how='left', suffixes=('', '_pre'))
        merged_df['update_type'] = merged_df.apply(
            lambda row: 2 if row['location'] != row['location_pre'] else row['update_type'],
            axis=1
        )

        updated_combined_up_df = pd.concat([merged_df, new_data], ignore_index=True)
        if not updated_combined_up_df.empty:
            updated_combined_up_df = updated_combined_up_df.sort_values(by=['rank', 'predicted_event_spend'],
                                                                        ascending=[False, False])
            updated_combined_up_df.to_csv(f'/tmp/NEWUP_TravelEvent_data.csv', index=False, encoding='utf-8-sig')
        else:
            updated_combined_up_df.to_csv(f'/tmp/NEWUP_TravelEvent_data.csv', index=False, encoding='utf-8-sig')
            print("No new data fetched. Skipping CSV creation.")

        print("New data fetched and saved to '/tmp/NEWUP_TravelEvent_data.csv'")
    else:
        print("No previous data found in S3. Skipping update.")
```

#### 해결 후 결과
- **API 호출 수 절감**: 업데이트된 정보만을 선택적으로 수집함으로써 API 호출 수를 대폭 줄일 수 있었습니다. 이는 API 키의 사용 한도를 효율적으로 관리하는 데 크게 기여했습니다.
- **데이터 정확성 향상**: 업데이트된 행사 리스트를 별도로 관리함으로써 최신의 정확한 데이터를 유지할 수 있었습니다. 이로 인해 서비스의 신뢰성과 사용자 경험이 향상되었습니다.

---

### 배운 점 및 교훈

이 과정을 통해 배운 것은 다음과 같습니다:
- **API 한도 관리**: API 호출 한도를 관리하기 위해 먼저 효율적인 데이터 수집 전략을 세우는 것이 중요하다는 것을 배웠습니다. API 호출 수를 줄이고, 데이터를 효율적으로 업데이트하는 방법을 통해 한도 문제를 해결할 수 있었습니다.
- **업데이트 전략의 중요성**: 데이터 업데이트 시 적절한 전략을 사용하여 호출 수를 줄이고, 필요한 정보만을 효과적으로 관리하는 것이 중요하다는 교훈을 얻었습니다. 이를 통해 시스템의 안정성과 데이터의 최신성을 보장할 수 있었습니다.

#### 추가 개선 가능성
- **자동화된 스케줄링**: API 호출 빈도를 관리하기 위해 자동화된 스케줄링 시스템을 도입하여, 한도 초과를 방지하고 데이터 수집을 더욱 효율적으로 수행할 수 있습니다.
- **API 키 관리**: API 키의 사용량을 모니터링하고, 필요 시 키를 갱신하거나 추가적인 키를 확보하여 한도 문제를 사전에 예방할 수 있습니다.

---

### 결론

API 키 한도 제한 문제를 해결하기 위한 접근 방식을 통해 많은 교훈을 얻었으며, 데이터 수집 및 업데이트의 효율성을 높일 수 있었습니다. 이 글이 비슷한 문제를 겪는 분들에게 도움이 되길 바랍니다.

---

### Q&A

**Q: API 호출 한도를 줄이기 위해 다른 방법은 무엇이 있나요?**

**A:** API 호출 한도를 줄이기 위해 다양한 방법이 있습니다. 예를 들어, 호출 빈도를 조절하거나, 데이터 캐싱을 통해 중복 호출을 줄이는 방법이 있습니다. 또한, 데이터의 필터링을 통해 필요한 정보만을 가져오는 것도 효과적입니다. 이 외에도 API 제공자의 요청에 따라 호출 한도를 조정하거나, 더 높은 한도의 요금제나 플랜을 고려할 수도 있습니다. 하지만 이번 프로젝트에서는 실습을 목적으로 하는 프로젝트인 만큼 요금제나 플랜을 사용하는 방법보다 코드를 사용해서 API의 호출 수를 줄이는 방법을 택했습니다.

**Q: 왜 호출 수를 줄이는 데에 업데이트 CSV를 따로 생성하는 방식을 사용했나요? 다른 효율적인 방법이 있지 않았을까요?**

**A:** 업데이트된 데이터를 별도의 CSV로 관리하는 방법을 선택한 이유는, 이 방법이 API 호출 수를 줄이는 동시에 데이터의 최신성과 정확성을 유지하는 데 가장 효과적이었기 때문입니다. 업데이트된 행사 정보만을 선택적으로 가져옴으로써, 불필요한 데이터를 반복적으로 요청하는 것을 방지할 수 있었습니다. 또한, 프로젝트의 구성상 데이터의 무결성을 유지하기 위해 행사와 관련된 데이터(근처 식당, 숙소, 항공권 등)는 S3에서 행사 데이터를 가져오는 방식을 사용하고 있습니다. 이러한 이유로, Redshift에 새로운 테이블을 생성하는 것보다 S3에 업데이트 내역이 담긴 CSV 파일을 저장하는 것이 더 효율적이라고 판단했습니다.

---

## 세 번째 이슈: Apache Airflow를 실행 중 리소스 부족 문제

### 문제의 배경
- **리소스 부족 문제**: 단일 인스턴스로 Apache Airflow를 실행하면서 CPU 및 메모리 자원이 부족해져 인스턴스가 중지되거나 종료되는 문제가 발생했습니다. 이로 인해 DAG의 실행 작업 시간이 길어지고 전체적인 처리 지연이 발생했습니다.
- **작업 처리 지연**: 리소스 부족으로 인해 DAG의 태스크가 순차적으로 실행되었고, 이로 인해 전체 워크플로우의 성능이 저하되었습니다.

**해결하지 않을 시**: 리소스 부족 문제를 해결하지 않으면 Apache Airflow의 안정적인 실행이 어려워지고, 데이터 처리 및 변환 작업의 지연이 계속될 수 있습니다.

### 시도한 해결 방법
- **워커 인스턴스를 별도의 인스턴스로 할당**: 기존의 단일 인스턴스에서 모든 Airflow 작업을 처리하는 대신, 워커 인스턴스를 별도로 정의하고 인스턴스 수를 늘려 리소스 부족 문제를 해결하려 했습니다. 이를 통해 추가적인 컴퓨팅 리소스를 확보하여 워크플로우 처리의 안정성을 높였습니다.
- **TaskGroup을 사용한 코드 개선**: Airflow의 TaskGroup 기능을 활용하여 특정 태스크를 병렬로 실행하도록 코드 구조를 최적화했습니다. 이로 인해 작업 속도를 향상시키고, 리소스를 유연하게 분배할 수 있게 되었습니다.

---

### 세 번째 이슈 해결 과정

#### 최종 해결 방법: 워커 인스턴스 분리 및 TaskGroup 활용

##### 워커 인스턴스 분리:
- **문제**: 단일 인스턴스에서 모든 Airflow 작업을 처리하면서 발생한 리소스 부족 문제.
- **해결책**: 워커 인스턴스를 별도로 정의하고 인스턴스 수를 증가시켜 리소스를 분산시켰습니다. 이를 통해 개별 인스턴스의 부하를 줄이고, 전체 워크플로우의 안정성을 향상시킬 수 있었습니다.

##### TaskGroup 활용:
- **문제**: 태스크가 순차적으로 실행되어 전체 작업 속도가 느려짐.
- **해결책**: Airflow의 TaskGroup 기능을 사용하여 태스크를 병렬로 실행하도록 코드 구조를 개선했습니다. 이를 통해 작업 속도를 크게 향상시켰고, 리소스를 보다 유연하게 분배할 수 있게 되었습니다.

#### 해결 후 결과
- **리소스 부족 해결**: 워커 인스턴스를 분리하고 인스턴스 수를 늘림으로써 리소스 부족 문제를 해결했습니다. 인스턴스 중지나 종료 없이 안정적으로 Airflow를 운영할 수 있게 되었습니다.
- **작업 속도 및 효율성 향상**: TaskGroup을 사용하여 태스크를 병렬로 처리함으로써 작업 속도가 크게 향상되었고, 유연한 리소스 분배로 전체적인 워크플로우 성능이 개선되었습니다.

---

### 배운 점 및 교훈

이 과정을 통해 배운 것은 다음과 같습니다:
- **리소스 분산의 중요성**: 단일 인스턴스에서 모든 작업을 처리하는 대신, 워커 인스턴스를 분리하고 리소스를 분산시키는 것이 안정성과 성능을 높이는 데 매우 중요하다는 것을 배웠습니다. 리소스를 효율적으로 분배함으로써 시스템의 성능을 크게 향상시킬 수 있었습니다.
- **병렬 처리의 효과**: Airflow의 TaskGroup 기능을 활용하여 태스크를 병렬로 실행함으로써 작업 속도와 효율성을 크게 개선할 수 있었습니다. 병렬 처리는 대규모 데이터 처리와 복잡한 워크플로우에서 성능을 높이는 효과적인 방법이라는 교훈을 얻었습니다.

#### 추가 개선 가능성
- **모니터링 및 경고 시스템 강화**: 리소스 사용량을 실시간으로 모니터링하고, 문제가 발생하기 전에 조기에 경고를 받을 수 있는 시스템을 도입하여 리소스 부족 문제를 미연에 방지할 수 있습니다.
- **자동 스케일링**: 클라우드 환경에서 자동 스케일링을 통해 리소스 수요에 따라 인스턴스를 동적으로 조정함으로써, 필요에 따라 추가적인 리소스를 확보하고 리소스 부족 문제를 방지할 수 있습니다.

---

### 결론

리소스 부족 문제를 해결하기 위한 접근 방식을 통해 많은 교훈을 얻었으며, Apache Airflow의 성능을 최적화하고 안정적인 실행을 보장할 수 있었습니다. 이 글이 비슷한 문제를 겪는 분들에게 도움이 되길 바랍니다.

### 추가 자료
- [Apache Airflow Documentation](https://airflow.apache.org/docs/apache-airflow/stable/)
- [AWS EC2 Auto Scaling Documentation](https://docs.aws.amazon.com/autoscaling/index.html)

---

### Q&A

**Q: 워커 인스턴스를 별도로 분리한 이유는 무엇인가?**

**A:** 워커 인스턴스를 별도로 분리한 이유는 단일 인스턴스에서 모든 작업을 처리할 경우 리소스 부족 문제가 발생할 수 있기 때문입니다. 별도의 인스턴스를 할당함으로써 리소스를 분산시키고, 각 인스턴스의 부하를 줄여 안정적이고 효율적인 작업 처리를 보장할 수 있었습니다.

**Q: TaskGroup을 사용한 이유와 효과는 무엇인가?**

**A:** TaskGroup을 사용한 이유는 태스크를 병렬로 실행하여 전체 작업 속도를 향상시키기 위해서입니다. TaskGroup을 활용함으로써 특정 태스크들을 동시에 실행할 수 있어 작업 처리 속도가 빨라졌고, 리소스를 보다 효율적으로 분배할 수 있었습니다. 이는 전체 워크플로우의 성능을 개선하는 데 큰 도움이 되었습니다.

---

